; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple riscv64 -mattr +experimental-v -o - %s \
; RUN:     | FileCheck %s

; This test file referenced the epi's gep-tuple test file
declare <vscale x 1 x i64> @llvm.riscv.vmv.v.x(i64);

%struct.__rvv_1x64x2 = type { <vscale x 1 x i64>, <vscale x 1 x i64> }

define <vscale x 1 x i64> @n1fv6() nounwind {
; CHECK-LABEL: n1fv6:
; CHECK: # %bb.0:  # %entry
; CHECK-NEXT:	addi	sp, sp, -32
; CHECK-NEXT:	sd	ra, 24(sp)
; CHECK-NEXT:	sd	s0, 16(sp)
; CHECK-NEXT:	addi	s0, sp, 32
; CHECK-NEXT:	csrr	a0, vlenb
; CHECK-NEXT:	slli	a0, a0, 1
; CHECK-NEXT:	sub	sp, sp, a0
; CHECK-NEXT:	sd	sp, -24(s0)
; CHECK-NEXT:	addi	a0, zero, 2
; CHECK-NEXT:	vsetvli	zero, zero, e64,m1,tu,mu
; CHECK-NEXT:	vmv.v.x	v16, a0
; CHECK-NEXT:	ld	a0, -24(s0)
; CHECK-NEXT:	vs1r.v	v16, (a0)
; CHECK-NEXT:	csrr	a1, vlenb
; CHECK-NEXT:	srli	a1, a1, 3
; CHECK-NEXT:	addi	a2, zero, 8
; CHECK-NEXT:	mul	a1, a1, a2
; CHECK-NEXT:	add	a0, a0, a1
; CHECK-NEXT:	vs1r.v	v16, (a0)
; CHECK-NEXT:	addi	sp, s0, -32
; CHECK-NEXT:	ld	s0, 16(sp)
; CHECK-NEXT:	ld	ra, 24(sp)
; CHECK-NEXT:	addi	sp, sp, 32
; CHECK-NEXT:	ret
entry:
  %KP500000000 = alloca %struct.__rvv_1x64x2, align 8

  %tmp.1.i305 = tail call <vscale x 1 x i64> @llvm.riscv.vmv.v.x(i64 2)
  %tmp.6 = getelementptr inbounds %struct.__rvv_1x64x2, %struct.__rvv_1x64x2* %KP500000000, i64 0, i32 0
  store <vscale x 1 x i64> %tmp.1.i305, <vscale x 1 x i64>* %tmp.6, align 8
  %tmp.8 = getelementptr inbounds %struct.__rvv_1x64x2, %struct.__rvv_1x64x2* %KP500000000, i64 0, i32 1
  store <vscale x 1 x i64> %tmp.1.i305, <vscale x 1 x i64>* %tmp.8, align 8

  %tmp.375 = load <vscale x 1 x i64>, <vscale x 1 x i64>* %tmp.8, align 8
  ret <vscale x 1 x i64> %tmp.375
}

%struct.__rvv_1xi64x4 = type { <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64> }

define <vscale x 1 x i64> @n1fv6_1() nounwind {
; CHECK-LABEL: n1fv6_1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:	addi	sp, sp, -32
; CHECK-NEXT:	sd	ra, 24(sp)
; CHECK-NEXT:	sd	s0, 16(sp)
; CHECK-NEXT:	addi	s0, sp, 32
; CHECK-NEXT:	csrr	a0, vlenb
; CHECK-NEXT:	slli	a0, a0, 2
; CHECK-NEXT:	sub	sp, sp, a0
; CHECK-NEXT:	sd	sp, -24(s0)
; CHECK-NEXT:	addi	a0, zero, 64
; CHECK-NEXT:	vsetvli	zero, zero, e64,m1,tu,mu
; CHECK-NEXT:	vmv.v.x	v25, a0
; CHECK-NEXT:	ld	a0, -24(s0)
; CHECK-NEXT:	vs1r.v	v25, (a0)
; CHECK-NEXT:	csrr	a1, vlenb
; CHECK-NEXT:	srli	a1, a1, 3
; CHECK-NEXT:	addi	a2, zero, 8
; CHECK-NEXT:	mul	a2, a1, a2
; CHECK-NEXT:	add	a2, a0, a2
; CHECK-NEXT:	vs1r.v	v25, (a2)
; CHECK-NEXT:	addi	a2, zero, 16
; CHECK-NEXT:	mul	a2, a1, a2
; CHECK-NEXT:	add	a2, a0, a2
; CHECK-NEXT:	vs1r.v	v25, (a2)
; CHECK-NEXT:	addi	a3, zero, 24
; CHECK-NEXT:	mul	a1, a1, a3
; CHECK-NEXT:	add	a0, a0, a1
; CHECK-NEXT:	vs1r.v	v25, (a0)
; CHECK-NEXT:	vl1re64.v	v16, (a2)
; CHECK-NEXT:	addi	sp, s0, -32
; CHECK-NEXT:	ld	s0, 16(sp)
; CHECK-NEXT:	ld	ra, 24(sp)
; CHECK-NEXT:	addi	sp, sp, 32
; CHECK-NEXT:	ret
entry:
  %KP500000000 = alloca %struct.__rvv_1xi64x4, align 8

  %tmp.1.i305 = tail call <vscale x 1 x i64> @llvm.riscv.vmv.v.x(i64 64)

  %tmp.6 = getelementptr inbounds %struct.__rvv_1xi64x4, %struct.__rvv_1xi64x4* %KP500000000, i64 0, i32 0
  store <vscale x 1 x i64> %tmp.1.i305, <vscale x 1 x i64>* %tmp.6, align 8
  %tmp.8 = getelementptr inbounds %struct.__rvv_1xi64x4, %struct.__rvv_1xi64x4* %KP500000000, i64 0, i32 1
  store <vscale x 1 x i64> %tmp.1.i305, <vscale x 1 x i64>* %tmp.8, align 8
  %tmp.8.2 = getelementptr inbounds %struct.__rvv_1xi64x4, %struct.__rvv_1xi64x4* %KP500000000, i64 0, i32 2
  store <vscale x 1 x i64> %tmp.1.i305, <vscale x 1 x i64>* %tmp.8.2, align 8
  %tmp.8.3 = getelementptr inbounds %struct.__rvv_1xi64x4, %struct.__rvv_1xi64x4* %KP500000000, i64 0, i32 3
  store <vscale x 1 x i64> %tmp.1.i305, <vscale x 1 x i64>* %tmp.8.3, align 8

  %tmp.375 = load <vscale x 1 x i64>, <vscale x 1 x i64>* %tmp.8.2, align 8
  ret <vscale x 1 x i64> %tmp.375
}